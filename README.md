# History of Massachusetts Q&A Bot

## Overview

The **History of Massachusetts Q&A Bot** is a Retrieval-Augmented Generation (RAG) system designed to answer questions about Massachusetts history. It leverages Wikipedia as the primary knowledge source, employs vector embeddings for semantic search, and integrates OpenAI's GPT-3.5-turbo for natural language responses. The system processes historical data into Markdown and CSV formats and uses a Streamlit interface for user interaction.

## Features

- **Data Collection**: Extracts structured data from Wikipedia using the `wikipedia-api` library.
- **Semantic Search**: Utilizes FAISS for fast and accurate similarity-based document retrieval.
- **Concise Answers**: GPT-3.5-turbo generates precise responses to user queries.
- **Interactive Frontend**: Streamlit provides an accessible interface for user interaction.

## File Structure

- **`Wiki_Scraper.py`**: Script for scraping and processing Wikipedia pages. Saves data as CSV and Markdown files. 
- **`RAGChat.py`**: Core implementation of the RAG system, including document chunking, embedding generation, and retrieval logic. 
- **`app.py`**: Streamlit application for hosting the Q&A bot. Handles user queries and displays responses interactively. 
- **`requirements.txt`**: Lists Python dependencies required for the project. 
- **`output.md`**: Example of Markdown output generated by the Wikipedia scraper. 

## Installation

### Prerequisites
- Python 3.8 or higher
- OpenAI API key

### Setup Steps
1. **Clone the Repository**:
   ```bash
   git clone <repository_url>
   cd <repository_folder>
   ```

2. **Install Dependencies**:
   Use `pip` to install the required libraries:
   ```bash
   pip install -r requirements.txt
   ```

3. **Configure OpenAI API**:
   Set your OpenAI API key in the Streamlit secrets file:
   ```bash
   mkdir -p ~/.streamlit
   echo "[secrets]\napi='<YOUR_API_KEY>'" > ~/.streamlit/secrets.toml
   ```

## Usage

### Data Preparation
1. **Scrape and Process Data**:
   Use `Wiki_Scraper.py` to scrape relevant Wikipedia pages and save the data in CSV and Markdown formats:
   ```bash
   python Wiki_Scraper.py
   ```

### Running the Application
1. **Start the Streamlit Interface**:
   Launch the Q&A bot using:
   ```bash
   streamlit run app.py
   ```

2. **Interact with the Bot**:
   Enter historical queries and receive concise, contextually accurate answers.

### Example Query
**Input**: "What is the history of the Massachusetts Bay Colony?"  
**Output**: Founded in 1630, became Province of Massachusetts Bay in 1691â€“92

## Key Components

### Data Scraping
- Extracts content from Wikipedia.
- Converts processed content into CSV and Markdown formats for downstream use.

### Semantic Search and RAG Pipeline
- **Embeddings**: HuggingFace's `all-MiniLM-L6-v2` for semantic encoding.
- **Retriever**: FAISS for high-speed similarity search across document chunks.
- **Compressor**: LLMChainExtractor to condense retrieved content before passing it to the language model.

### Language Model
- **Model**: GPT-3.5-turbo via OpenAI API.
- **Function**: Generates natural language responses based on compressed, contextually relevant data.

## Dependencies
Key libraries and tools:
- `langchain`, `faiss-cpu`, `wikipedia-api`, `streamlit`, `openai`, `transformers`, `sentence-transformers`.

Refer to `requirements.txt` for the full list of dependencies.


## Troubleshooting
- **API Issues**: Ensure the OpenAI API key is valid and set up correctly in Streamlit secrets.
- **Vector Store**: Verify FAISS index is correctly initialized with the processed data.
- **UI Errors**: Restart the Streamlit server and check the console logs for detailed error messages.
